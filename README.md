# Differential-MOE
ðŸš€ Building a Differential Transformer with Mixture of Experts (MoE) from scratch. ~18-22B sparse parameters, FP8 quantization, custom CUDA kernels for AMD MI300X. Learning project exploring modern LLM architectures, efficient training, and MoE routing.
