
# Hardware / Runtime
max_batch_size: 16
max_seq_len: 8192
dtype: fp8                # MI300X loves fp8
scale_fmt: per_tensor     # safer than per_channel for training


# Tokenizer / Embeddings
vocab_size: 32000
dim: 6144                 # ~6B-class backbone width
inter_dim: 24576          # 4x dim (standard Transformer)
moe_inter_dim: 32768      # wider experts for specialization

# Transformer Stack
n_layers: 48
n_dense_layers: 8         # first layers dense, rest MoE
n_heads: 48               # head_dim ≈ 128
index_head_dim: 128
index_topk: 4


# Mixture of Experts
n_routed_experts: 64
n_shared_experts: 2
n_activated_experts: 4
n_expert_groups: 8
n_limited_groups: 4
score_func: softmax
route_scale: 1.0


# Attention Dimensions
qk_nope_head_dim: 64
qk_rope_head_dim: 64
v_head_dim: 128


# LoRA (memory-efficient finetuning)
q_lora_rank: 16
kv_lora_rank: 8


# Rotary Positional Encoding
original_seq_len: 4096
rope_theta: 10000.0
rope_factor: 4.0           # enables 8k context safely


# Extended Attention Scaling
beta_fast: 32
beta_slow: 64
mscale: 1.0

# Notes

# Estimated footprint (fp8):
# - Parameters: ~18–22B (MoE sparse active)
# - Activation + optimizer states fit within 192GB
# - Designed for single MI300X without tensor parallelism
